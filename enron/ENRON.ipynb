{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-02 10:24:54--  https://storage.googleapis.com/reinfer-datasets/enron_mail_20150507.tar.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.210.48, 2a00:1450:4009:807::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.210.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 443254787 (423M) [application/x-tar]\n",
      "Saving to: ‘enron_mail_20150507.tar.gz’\n",
      "\n",
      "enron_mail_20150507 100%[===================>] 422.72M   109MB/s    in 4.5s    \n",
      "\n",
      "2019-06-02 10:24:58 (93.0 MB/s) - ‘enron_mail_20150507.tar.gz’ saved [443254787/443254787]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://storage.googleapis.com/reinfer-datasets/enron_mail_20150507.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tar -xf enron_mail_20150507.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wrangle the data to understand the relationships between senders and recipients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from collections import defaultdict, Counter\n",
    "from mailbox import mboxMessage\n",
    "\n",
    "# What we want to do here is create a data-structure that maps from: recieved : n\n",
    "# to do this efficiently we use the defaultdict-counter combo\n",
    "# I experimented a little using the 'mbox' class to do everything at once but couldn't make it work. If I have time i'll\n",
    "# go back\n",
    "\n",
    "# Certain emails don't have 'To' or 'From' usually ones that are going to a group list. \n",
    "# As we don't know who is on these lists lets ignore this class of message for now, even \n",
    "# though this isn't ideal, as it is conceivable that big influencers are much more likely to \n",
    "# emails these lists\n",
    "            \n",
    "# Currently we negelect cc's and bbc's for purely compuational reasons, though\n",
    "# there is no other reason not to include them\n",
    "\n",
    "# data structures:\n",
    "# Effectively a graph, and in reality one would just build a graph using a standard library and run HITS on that. Here\n",
    "# log both the incoming and outgoing edges independently for each of use in the custom HITS implementation\n",
    "outgoing = defaultdict(Counter) # outgoingconnection counts\n",
    "incoming = defaultdict(Counter) # incoming connection counts\n",
    "\n",
    "\n",
    "previously_seen = set() # keeps a hash of the payloads of previously seen messages to avoid double counting\n",
    "\n",
    "for f in glob.glob(\"maildir/**\", recursive=True):\n",
    "    try:\n",
    "        with open(f) as mbox_file:\n",
    "            msg = mboxMessage(mbox_file)\n",
    "                \n",
    "            payload = msg.get_payload()\n",
    "            if  msg[\"From\"] is not None and msg[\"To\"] is not None and payload is not None:\n",
    "                payload_hash = hash(payload)\n",
    "                if payload_hash not in previously_seen:\n",
    "                    fr = msg[\"From\"]\n",
    "                    to = re.sub('\\ |\\n|\\t', '', msg[\"To\"]).split(\",\") # remove special characters and spaces\n",
    "                    outgoing[fr].update([person for person in to if person != fr]) # important to remove self edges as these are a big source of noise\n",
    "                    for person in to:\n",
    "                        if person != fr:\n",
    "                            incoming[person].update([fr]) \n",
    "                    previously_seen.add(payload_hash)\n",
    "            \n",
    "    except (IsADirectoryError, UnicodeDecodeError) as e:\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19567 unique senders\n"
     ]
    }
   ],
   "source": [
    "print (\"{} unique senders\".format(len(connections)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of Hubs and Authorities\n",
    "\n",
    "Just implement a vanilla HA without regards for performance. I'm not sure how long it will take to run, so if its slow i'll optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Score:\n",
    "    __slots__ = 'hub', 'auth'\n",
    "    def __init__(self, hub=1, auth=1):\n",
    "        self.hub = hub\n",
    "        self.auth= auth ## todo: check the initialization of slots\n",
    "\n",
    "ha_scores = {email:Score() for email in list(outgoing.keys()) + list(incoming.keys())}  # email: [hub score, authority scores]\n",
    "\n",
    "def auth_update(scores, incoming_edges, weight_transformation):\n",
    "    norm = 0\n",
    "    for person in scores.keys():\n",
    "        scores[person].auth = 0\n",
    "        for connection, weight in incoming_edges[person].items():\n",
    "            \n",
    "            scores[person].auth += weight_transformation(weight)*scores[connection].hub\n",
    "        norm += scores[person].auth**2\n",
    "            \n",
    "    norm = norm**0.5\n",
    "    for person in scores.keys():\n",
    "        scores[person].auth /= norm\n",
    "        \n",
    "def hub_update(scores, outgoing_edges, weight_transformation):\n",
    "    norm = 0\n",
    "    for person in scores.keys():\n",
    "        scores[person].hub = 0\n",
    "        for connection, weight in outgoing_edges[person].items():\n",
    "            scores[person].hub += weight_transformation(weight)*scores[connection].auth\n",
    "        norm += scores[person].hub**2\n",
    "            \n",
    "    norm = norm**0.5\n",
    "    for person in scores.keys():\n",
    "        scores[person].hub /= norm\n",
    "        \n",
    "        \n",
    "def hits(scores,\n",
    "         outgoing_edges,\n",
    "         incoming_edges,\n",
    "         weight_transformation=lambda weight:weight,\n",
    "         max_iter=100):\n",
    "    \"\"\"\n",
    "    Update HITS hubs and authorities values for nodes.\n",
    "    \n",
    "    :param scores : The hubs and authorities scores for each node\n",
    "                    in the graph. Hubs score calculated on outgoing\n",
    "                    connections and authorities score calculated\n",
    "                    from incoming connections.\n",
    "    \n",
    "    :param incoming_edges: Incoming edges and associated counts \n",
    "    \n",
    "    :param outgoing_edges: Out edges and associated counts\n",
    "    \n",
    "    :param weight_transformation: Function applied to the raw\n",
    "                     counts to derive the weights\n",
    "    \n",
    "    :param max_iter: Number of iterations the algorithm runs for.\n",
    "                     Note, we currently don't check for convergence\n",
    "                     and an improvement to this algorithm could be\n",
    "                     to perform such an action.       \n",
    "    \"\"\"\n",
    "    for iteration in range(max_iter):\n",
    "        print (\"Running iteration\",iteration,end=\"\\r\")\n",
    "        auth_update(scores, incoming_edges, weight_transformation)\n",
    "        hub_update(scores, outgoing_edges, weight_transformation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run HITS and find the influential people in the organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 99\r"
     ]
    }
   ],
   "source": [
    "hits(ha_scores, outgoing, incoming)\n",
    "authority_scores = sorted([(person, score.auth) for person, score in ha_scores.items()], key=lambda x:x[1])\n",
    "hub_scores = sorted([(person, score.hub) for person, score in ha_scores.items()], key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mpalmer@enron.com', 0.13138844701950742),\n",
       " ('alan.comnes@enron.com', 0.14488565586942323),\n",
       " ('skean@enron.com', 0.1520301828018625),\n",
       " ('sandra.mccubbin@enron.com', 0.16583307696183436),\n",
       " ('harry.kingerski@enron.com', 0.1828764769951093),\n",
       " ('karen.denne@enron.com', 0.22208054837876512),\n",
       " ('james.steffes@enron.com', 0.2546958855676058),\n",
       " ('paul.kaufman@enron.com', 0.28180420772830467),\n",
       " ('susan.mara@enron.com', 0.28202716121445387),\n",
       " ('richard.shapiro@enron.com', 0.29244369621562544)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authority_scores[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('karen.denne@enron.com', 0.041451846701707165),\n",
       " ('sgovenar@govadv.com', 0.04875516915071102),\n",
       " ('d..steffes@enron.com', 0.04894466242855275),\n",
       " ('alan.comnes@enron.com', 0.06781710065529507),\n",
       " ('miyung.buster@enron.com', 0.06918860433321587),\n",
       " ('james.steffes@enron.com', 0.07136241435638094),\n",
       " ('mary.hain@enron.com', 0.07209525253439127),\n",
       " ('ginger.dernehl@enron.com', 0.1631010334561144),\n",
       " ('susan.mara@enron.com', 0.3035096098153337),\n",
       " ('jeff.dasovich@enron.com', 0.9174549003982896)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_scores[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging into these scores - the importance of the edge weights\n",
    "\n",
    "Looking at these people, it seems like they aren't necessarily influential people in the company, but may perform tasks that require them to be cc'd in on a lot of emails (or send them). It currently appears as through the algorithm is placing too much emphasis on the number of emails sent. Consequently, we experiment we a callback passed into the HITS algorithm that calculates the weights from the email counts. Due to time constraints, the only callback currently tested is the extreme case that binarizes weights: 1 if present and 0 if not. Below are the results calculated for this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 99\r"
     ]
    }
   ],
   "source": [
    "hits(ha_scores, outgoing, incoming, weight_transformation=lambda c: (bool(c)))\n",
    "authority_scores = sorted([(person, score.auth) for person, score in ha_scores.items()], key=lambda x:x[1])\n",
    "hub_scores = sorted([(person, score.hub) for person, score in ha_scores.items()], key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mark.haedicke@enron.com', 0.07488758142809376),\n",
       " ('tana.jones@enron.com', 0.07552716732968225),\n",
       " ('mark.taylor@enron.com', 0.07895266309179885),\n",
       " ('tim.belden@enron.com', 0.08083804773983622),\n",
       " ('steven.kean@enron.com', 0.0816167001256113),\n",
       " ('elizabeth.sager@enron.com', 0.08242413411150346),\n",
       " ('sally.beck@enron.com', 0.08918405915572611),\n",
       " ('greg.whalley@enron.com', 0.0933039061254153),\n",
       " ('john.lavorato@enron.com', 0.10744838163419328),\n",
       " ('louise.kitchen@enron.com', 0.12264309889716965)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authority_scores[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maxine.levingston@enron.com', 0.10223140028217378),\n",
       " ('daniel.muschar@enron.com', 0.10303974818686064),\n",
       " ('technology.enron@enron.com', 0.12042261529136816),\n",
       " ('nicki.daw@enron.com', 0.12087279895445646),\n",
       " ('billy.lemmons@enron.com', 0.12609532448743777),\n",
       " ('david.oxley@enron.com', 0.12858231679410545),\n",
       " ('outlook.team@enron.com', 0.14392841207707993),\n",
       " ('kenneth.lay@enron.com', 0.14907545981705006),\n",
       " ('sally.beck@enron.com', 0.1587743105635194),\n",
       " ('david.forster@enron.com', 0.18663705607163994)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_scores[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly, these people seem to be more important to the company operations. Looking at the top authorities, for example yields former head of trading operations,John Lavorato, ranking second. Greg Whalley was former president and clearly another big player. Qualitatively, the big hubs don't seem to be nearly as 'authoritative' (defined in the conventioanl sense), which makes sense. These seem to be people, or group emails, that were involved in large amounts administrative communication: necessary functions for the company but certainly not ranking as high in the chain of command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What did the important people talk about differentially?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive approach could be to perform an enrichment analysis between the word frequencies in the top ranked and bottom ranked hubs and authorities. Alternatively, framing this as a classification problem and trying to discriminate between these two groups could prove fruitful. Certainly, certain classical machine learning algorithms allow for easy interpretation of feature importance (i.e. weights in a logistic regression or gini impurity importance in random forests). For more complex models, mean decrease accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default-python3",
   "language": "python",
   "name": "default-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
