{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-02 10:24:54--  https://storage.googleapis.com/reinfer-datasets/enron_mail_20150507.tar.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.210.48, 2a00:1450:4009:807::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.210.48|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 443254787 (423M) [application/x-tar]\n",
      "Saving to: ‘enron_mail_20150507.tar.gz’\n",
      "\n",
      "enron_mail_20150507 100%[===================>] 422.72M   109MB/s    in 4.5s    \n",
      "\n",
      "2019-06-02 10:24:58 (93.0 MB/s) - ‘enron_mail_20150507.tar.gz’ saved [443254787/443254787]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://storage.googleapis.com/reinfer-datasets/enron_mail_20150507.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tar -xf enron_mail_20150507.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wrangle the data to get sender : reciepient mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from collections import defaultdict, Counter\n",
    "from mailbox import mboxMessage\n",
    "\n",
    "# What we want to do here is create a data-structure that maps from: recieved : n\n",
    "# to do this efficiently we use the defaultdict-counter combo\n",
    "# I experimented a little using the 'mbox' class to do everything at once but couldn't make it work. If I have time i'll\n",
    "# go back\n",
    "\n",
    "# Certain emails don't have 'To' or 'From' usually ones that are going to a group list. \n",
    "# As we don't know who is on these lists lets ignore this class of message for now, even \n",
    "# though this isn't ideal, as it is conceivable that big influencers are much more likely to \n",
    "# emails these lists\n",
    "            \n",
    "\n",
    "# data structures:\n",
    "# Effectively a graph, and in reality one would just build a graph using a standard library and run HITS on that. Here\n",
    "# log both the incoming and outgoing edges independently for each of use in the custom HITS implementation\n",
    "outgoing = defaultdict(Counter) # outgoingconnection counts\n",
    "incoming = defaultdict(Counter) # incoming connection counts\n",
    "\n",
    "\n",
    "previously_seen = set() # keeps a hash of the payloads of previously seen messages to avoid double counting\n",
    "\n",
    "for f in glob.glob(\"maildir/**\", recursive=True):\n",
    "    try:\n",
    "        with open(f) as mbox_file:\n",
    "            msg = mboxMessage(mbox_file)\n",
    "                \n",
    "            payload = msg.get_payload()\n",
    "            if  msg[\"From\"] is not None and msg[\"To\"] is not None and payload is not None:\n",
    "                payload_hash = hash(payload)\n",
    "                if payload_hash not in previously_seen:\n",
    "                    fr = msg[\"From\"]\n",
    "                    to = re.sub('\\ |\\n|\\t', '', msg[\"To\"]).split(\",\") # remove special characters and spaces\n",
    "                    outgoing[fr].update(to)\n",
    "                    for person in to:\n",
    "                        incoming[person].update([fr]) \n",
    "                    previously_seen.add(payload_hash)\n",
    "            \n",
    "    except (IsADirectoryError, UnicodeDecodeError) as e:\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19567 unique senders\n"
     ]
    }
   ],
   "source": [
    "print (\"{} unique senders\".format(len(connections)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation of Hubs and Authorities\n",
    "\n",
    "Just implement a vanilla HA without regards for performance. I'm not sure how long it will take to run, so if its slow i'll optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\r"
     ]
    }
   ],
   "source": [
    "class Score:\n",
    "    __slots__ = 'hub', 'auth'\n",
    "    def __init__(self, hub=1, auth=1):\n",
    "        self.hub = hub\n",
    "        self.auth= auth ## todo: check the initialization of slots\n",
    "\n",
    "ha_scores = {email:Score() for email in list(outgoing.keys()) + list(incoming.keys())}  # email: [hub score, authority scores]\n",
    "\n",
    "def auth_update(scores, incoming_edges):\n",
    "    norm = 0\n",
    "    for person in scores.keys():\n",
    "        scores[person].auth = 0\n",
    "        for connection in incoming_edges[person].keys():\n",
    "            \n",
    "            scores[person].auth += scores[connection].hub\n",
    "        norm += scores[person].auth**2\n",
    "            \n",
    "    norm = norm**0.5\n",
    "    for person in scores.keys():\n",
    "        scores[person].auth /= norm\n",
    "        \n",
    "def hub_update(scores, outgoing_edges):\n",
    "    norm = 0\n",
    "    for person in scores.keys():\n",
    "        scores[person].hub = 0\n",
    "        for connection in outgoing_edges[person].keys():\n",
    "            scores[person].hub += scores[connection].auth\n",
    "        norm += scores[person].hub**2\n",
    "            \n",
    "    norm = norm**0.5\n",
    "    for person in scores.keys():\n",
    "        scores[person].hub /= norm\n",
    "        \n",
    "        \n",
    "def hits(scores, outgoing_edges, incoming_edges, max_iter=100):\n",
    "    \"\"\"\n",
    "    Update HITS hubs and authorities values for nodes.\n",
    "    \n",
    "    :param scores : The hubs and authorities scores for each node\n",
    "                    in the graph. Hubs score calculated on outgoing\n",
    "                    connections and authorities score calculated\n",
    "                    from incoming connections.\n",
    "    \n",
    "    :param incoming_edges: Incoming edges and associated weights \n",
    "    \n",
    "    :param outgoing_edges: Out edges and associated weights\n",
    "    \n",
    "    :param max_iter: Number of iterations the algorithm runs for.\n",
    "                     Note, we currently don't check for convergence\n",
    "                     and an improvement to this algorithm could be\n",
    "                     to perform such an action.       \n",
    "    \"\"\"\n",
    "    for iteration in range(max_iter):\n",
    "        print (\"Running iteration\",iteration,end=\"\\r\")\n",
    "        auth_update(scores, incoming_edges)\n",
    "        hub_update(scores, outgoing_edges)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run HITS and find the influention people in the organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hits(ha_scores, outgoing, incoming)\n",
    "authority_scores = sorted([(person, score.auth) for person, score in ha_scores.items()], key=lambda x:x[1])\n",
    "hub_scores = sorted([(person, score.hub) for person, score in ha_scores.items()], key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mark.haedicke@enron.com', 0.0755408347767774),\n",
       " ('tana.jones@enron.com', 0.07640490087856729),\n",
       " ('mark.taylor@enron.com', 0.07950347900295653),\n",
       " ('tim.belden@enron.com', 0.08113851261947602),\n",
       " ('steven.kean@enron.com', 0.08213922127524408),\n",
       " ('elizabeth.sager@enron.com', 0.08288948053083063),\n",
       " ('sally.beck@enron.com', 0.0904662958015301),\n",
       " ('greg.whalley@enron.com', 0.09313554995012904),\n",
       " ('john.lavorato@enron.com', 0.10821510238262484),\n",
       " ('louise.kitchen@enron.com', 0.12346263617889451)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authority_scores[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daniel.muschar@enron.com', 0.10228356793369467),\n",
       " ('louise.kitchen@enron.com', 0.10294998798518347),\n",
       " ('technology.enron@enron.com', 0.11945685305893489),\n",
       " ('nicki.daw@enron.com', 0.1203082352208114),\n",
       " ('billy.lemmons@enron.com', 0.12527074520888876),\n",
       " ('david.oxley@enron.com', 0.12846915088138788),\n",
       " ('outlook.team@enron.com', 0.14305557927351475),\n",
       " ('kenneth.lay@enron.com', 0.14857182614134243),\n",
       " ('sally.beck@enron.com', 0.15875307608568873),\n",
       " ('david.forster@enron.com', 0.1859679999086267)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_scores[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default-python3",
   "language": "python",
   "name": "default-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
